스케일링 하는 이유
: 연산 데이터가 아주 크면 신경망 레이어를 거칠수록 용량때문에 터져버린다.
: 텐서연산이 부동소수점 연산에 특화되있다.(최대 1을 못넘음)

MaxScaler : 최대값으로 나눔. 데이터들을 1이하로 줄임.(데이터분포를 0~1로 줄임)
1,2,3,4,50,60,100 -> 0.01, 0.02, 0.03, 0.04, 0.5, 0.6, 1
850, 900, 1000 -> 0.85, 0.9, 1 : 최대값으로만 나누면 데이터범위중 0이 나오질 않는 문제가있다.

MinMaxScaler : x - min / max - min : 데이터 분포를 0과 1을 포함한 0~1 범위로 줄인다.
1,2,3,4,50,60,100 -> 1-1/100-1, 2-1/100-1, 3-1/100-99,..., 100-1/100-1

장점 : 아무리 큰 정수데이터라도 텐서연산에 특화된 부동소수점연산을 가능하게한다.

y=ax+b
스케일러만 하면 데이터(x)조작이지만 학습할때 목표값 y는 그대로이기때문에 학습조작이 아니다.
스케일러를 거친 x가 원래의 x가 가르키는 y(정답지)를 가르키면 문제-정답지 매칭은 같기때문에 모델을 만드는 학습에는 문제가없다.

부동소수점연산(아주 작은 값은 제거)으로 연산의 부담이 줄어든다.

딥러닝의 목표는 일차함수(직선)을 그리는게 목표다. 비선형적으로 분포된 데이터를 완벽히 매칭하는 꼬불꼬불한 모양의 함수를 그리게되면
새 데이터가 출현했을 경우 값을 예측못하는 문제가있다. 
부동소수점연산은 아주 작은 값을 버리면서 오차가 생길 수 밖에없다.
그러나 오차를 인정하면서 가장 최적화된 일직선 모양의 함수를 그려야 새로운 데이터도 작은 오차로 예측할 수있다.

수치화된 문자데이터같은건 스케일링을 적용하기에 애매하다. 숫자를 특정비율로 줄이는 개념을 그대로 적용하기 애매하다.

MinMaxScaler를 그냥 적용하면 컬럼별로 스케일링된다. 애초에 컬럼을 전부 합쳐서 범위지정하고 여기서 min, max 추출해서 스케일링하면 새로운 컬럼을 만들고 스케일링하는 것과 다름없다.

Feature Engineering : 컬럼의 데이터특성이 서로 다르면 동일한 스케일러를 쓰기 애매하다. 이 때는 각 컬럼에 적합한 스케일링을 따로해야한다.
(ex : MinMaxScaler('컬럼1'), StandardScaler('컬럼2') )

데이터불균형문제는 해결못하기때문에 데이터분포를 표준편차형태로 변형하는 StandardScaler방법도있다.

MinMaxScaler(정규화) : 이상치에 제일 민감. 최대값 최소값을 0~1사이로 압축(부동소수점 연산하는 신경망 연산에 유리)
StandardScaler(표준화) : 이상치에 덜 민감. 평균을 이용해 표준정규분포비슷한 형태로 바꿈. 범위가 0~1사이가 아님
RobustScaler : 이상치에 제일 강함. 중간값이나 사분위값을 써서 이상치를 반영하지않음. 범위가 0~1사이가 아님

(중요)스케일링은 train데이터에만 적용되어야한다.(test, validation 데이터에는 적용X)
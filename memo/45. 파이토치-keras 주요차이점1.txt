"""
######################################################################################################################
[TF, Sklearn, Pytorch 주요차이점 1 : (손실)함수 파라미터 순서]
 - TF, Sklearn : target, output 순
 - Pytorch : output, target 순

######################################################################################################################
[TF와 Pytorch 주요차이점 2 : 타겟라벨(y)전처리]
 - TF : y가 문자열이면 Label Encoding 등 정수 인코딩이 먼저 필요.
       - 손실 함수로 categorical_crossentropy 사용 시: y를 원-핫 인코딩 해야 함.
       - 손실 함수로 sparse_categorical_crossentropy 사용 시: y를 정수 인덱스 그대로 사용 (원-핫 인코딩 불필요).
 - PyTorch : 다중분류 시,
       - y는 반드시 0부터 시작하는 정수 인덱스(torch.long 타입)여야 함 (Label Encoding 필요)
       - 손실 함수로 nn.CrossEntropyLoss 사용 시: 
            -> 내부적으로 Softmax를 포함하므로, 출력층에는 활성화 함수를 사용X
            -> 내부적으로 sparse_categorical_crossentropy를 포함하므로, y 원-핫인코딩 사용X
 
#######################################################################################################################
[타겟라벨(y)의 종류에 따른 손실함수와 텐서타입]
 - 회귀 : MSELoss, torch.float32
 - 이진 : BCELoss, torch.float32
 - 다중 : CrossEntropyLoss, torch.long(=torch.int64)

 (참고) BCEWithLogitsLoss : BCELoss + Sigmoid
"""
######################################################################################################################
[파이토치 손실함수별 파라미터 순서 변경 시 영향 정리]
#
# ┌────────────┬──────────────────────────────────────────────────────────────────────────────┬────────────┬────────────┬───────────────────────────────────────────────────────────────┐
# │ 손실함수   │ 공식(수식)                                                                   │ 논리오류   │ 수식오류   │ 설명                                                          │
# ├────────────┼──────────────────────────────────────────────────────────────────────────────┼────────────┼────────────┼───────────────────────────────────────────────────────────────┤
# │ MSELoss    │ MSE(output, target) = (1/N)∑(output_i - target_i)^2                          │   X        │    X       │ 제곱 연산은 대칭 ⇒ 순서 바뀌어도 결과 같음                             │
# ├────────────┼──────────────────────────────────────────────────────────────────────────────┼────────────┼────────────┼───────────────────────────────────────────────────────────────┤
# │ BCELoss    │ BCE(p, t) = -(1/N)∑[ t_i*log(p_i) + (1 - t_i)*log(1 - p_i) ]                  │   O        │    X       │ 입력/타겟 역할이 다름. 순서 바뀌면 손실 해석 오류                         │
# │            │                                                                              │            │            │ (계산은 됨, 값은 무의미)                                             │
# ├────────────┼──────────────────────────────────────────────────────────────────────────────┼────────────┼────────────┼───────────────────────────────────────────────────────────────┤
# │ CrossEntropy│ CE(x, c) = -log( softmax(x)_c ) = -log( exp(x_c) / ∑_j exp(x_j) )         │   O        │    O       │ x: logits(float, 2D), c: 인덱스(long, 1D).                        │
# │   Loss      │ (내부적으로 softmax 연산 포함)                                               │            │            │ 순서 바꾸면 내부 softmax에 정수연산 시도 → 즉시 에러 발생                    │
# └────────────┴──────────────────────────────────────────────────────────────────────────────┴────────────┴────────────┴───────────────────────────────────────────────────────────────┘
#
# ▷ 반드시 criterion(output, target) 순서로 사용할 것!
# ▷ CrossEntropyLoss는 수식에 softmax 연산이 포함되어 있어, 모델 출력에는 softmax를 쓰지 않음.
# =========================================================

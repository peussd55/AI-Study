활성함수 = 활성화함수 = 한정함수
- [회귀]
1. linear
2. relu(디폴트) : 어떤 레이어의 입력신호 총합(시그마(ax)+b)이 음수일경우 모두 0으로 처리해버리는 함수

- [이진분류] : 출력레이어에는 무조건 sigmoid 사용한다.
1. sigmoid : y를 0~1 사이로 한정하는 함수 (x가 0일때는 y=0.5) 계단함수 계량

- [다중분류] : 출력레이어에는 무조건 softmax 사용한다.
1. softmax : 원핫인코딩을 거친 출력값(행렬)이 합하면 1이 되게한다. -> 가장 큰 값 하나를 1로 하고 나머지는 0으로 처리한다. -> 세 클래스를 대상으로 한다치면 [1, 0, 0] 또는 [0, 1, 0] 또는 [0, 0, 1] 로 바꾼다.
ex : 훈련을 한번 거쳐서 [1, 2, 3]가 나왔다 치면 
1/1+2+3, 2/1+2+3, 3/1+2+3(=0.5) 방법을 쓸 수도 있지만 1과 0으로 수렴하게 하려면 거쳐서 상대적인 수치차이를 극대화해야한다.
(-> 여기서 나오는 값이 커야 후에 categorical_crossentropy에서 loss 계산할때 작게나온다.)
e^1/e^1+e^2+e^3, e^2/e^1+e^2+e^3, e^3/e^1+e^2+e^3(= 0.665) 방법을 쓰면 값들의 차이가 극대화된다. (= 극대화될수록 상대적인 수치가 제일 큰 값은 1로 수렴하고 나머지는 0으로 수렴한다.)
(-> e^x1/e^x1+e^x2+e^x3이 1에 가까울수록 손실함수에서 계산한 loss가 0에 수렴한다.)
그리고 [-1, -1, 0] 같은 형태가 나올때 전자의 방법을 쓰면 분모가 0이 되는 문제가 있다.
=> e를 쓰는 이유 : 차이 극대화, 음수처리(음수때문에 분모 0이 되는 현상(=무한대가 되버리는 현상)제거)
Dropout하는 이유 : 과적합방지
(그럼 처음부터 layer를 적게하면되는거아님? -> 맨처음에 레이어가 몇개여야 학습잘되는지 모르므로 일단 늘리고 Dropout으로 조절하는 것임)

- dropout은 학습할때만 적용되고 테스트할때는 적용X

model.add(Dense(50, input_dim=14, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(25, activation='relu'))

-> 50개의 노드들이 각각 30%확률로 비활성화 
-> 비활성화된 노드에 연결된 가중치는 학습X 
-> 역전파때 가중치 업데이트 X 
-> 다음 학습때(다음 epochs때, 정확히는 다음 Batch 때) 비활성화 노드 랜덤으로 바뀜(30%확률로 비활성화가 다시 적용되므로)
-> 비활성화된 노드에 연결된 가중치 업데이트 안되고 다음 학습 반복
### <<65>>

import tensorflow as tf
tf.compat.v1.random.set_random_seed(777)

# 1. ë°ì´í„°
x_data = [[0,0], [0,1], [1,0], [1,1]]   # (2,2)
y_data = [[0], [1], [1], [0]]           # (2,1)

x = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])
y = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])

# w = tf.compat.v1.Variable(tf.random_normal([2,1], name='weights'))
# b = tf.compat.v1.Variable(tf.zeros([1], name='bias'))

rate = tf.compat.v1.placeholder(tf.float32)

# 2. ëª¨ë¸ (MLP êµ¬ì¡°ë¡œ ë³€ê²½)
# -----------------------------------------------------------------
# Layer 1: Hidden Layer (ì€ë‹‰ì¸µ)
# ì…ë ¥ íŠ¹ì„±(feature)ì´ 2ê°œ, íˆë“  ë…¸ë“œì˜ ê°œìˆ˜ë¥¼ 3ê°œë¡œ ì„¤ì •
w1 = tf.compat.v1.Variable(tf.random_normal([2, 3], name='weights1'))
b1 = tf.compat.v1.Variable(tf.zeros([3], name='bias1'))
# (N, 2) * (2, 3) -> (N, 3)
################ relu ì ìš© ################
# layer1 = tf.matmul(x, w1) + b1
layer1 = tf.nn.relu(tf.matmul(x, w1) + b1)   
# layer1 = tf.nn.selu(tf.matmul(x, w1) + b1)  
# layer1 = tf.nn.leaky_relu(tf.matmul(x, w1) + b1)               
# layer1 = tf.nn.elu(tf.matmul(x, w1) + b1)
#############################################


# Layer 2: Hidden Layer (ì€ë‹‰ì¸µ)
# Layer 1ì˜ ì¶œë ¥(3ê°œ)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… ê²°ê³¼(4ê°œ)ë¥¼ ì¶œë ¥
w2 = tf.compat.v1.Variable(tf.random_normal([3, 4], name='weights2'))
b2 = tf.compat.v1.Variable(tf.zeros([4], name='bias2'))
layer2 = tf.compat.v1.sigmoid(tf.matmul(layer1, w2) + b2)
# (N, 3) * (3, 4) -> (N, 4)
################ dropout ì ìš© ################
layer2 = tf.nn.dropout(layer2, rate=rate)
# layer2 = tf.nn.dropout(layer2, keep_prob=0.8)
#############################################

# Layer 3: Output Layer (ì¶œë ¥ì¸µ)
# Layer 2ì˜ ì¶œë ¥(4ê°œ)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… ê²°ê³¼(1ê°œ)ë¥¼ ì¶œë ¥
w3 = tf.compat.v1.Variable(tf.random_normal([4, 1], name='weights2'))
b3 = tf.compat.v1.Variable(tf.zeros([1], name='bias2'))
hypothesis = tf.compat.v1.sigmoid(tf.matmul(layer2, w3) + b3)   
# (N, 4) * (4, 1) -> (N, 1)

"""
ğŸŒ [ì¤‘ìš”] ëª¨ë¸ì˜ ë¹„ì„ í˜•ì„± ë¶€ì—¬ëŠ” ì€ë‹‰ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ì— ì˜í•´ì„œ ê²°ì •ëœë‹¤. ì¶œë ¥ì¸µì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ë¹„ì„ í˜•ì„±ê³¼ëŠ” ì•„ë¬´ì‚¬ ìƒê´€ì´ì—†ë‹¤.
-> íˆë“  ë ˆì´ì–´ ì¤‘ì— í•œ ê³³ì´ë¼ë„ sigmoidë¡œ ê°ì‹¸ì„œ ë¹„ì„ í˜•ì„±ì„ ì¤˜ì•¼ accê°€ 1.0ë‹¬ì„±í•œë‹¤.
 (1) : ì¶œë ¥ì¸µ ì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜(sigmoid) : ìµœì¢… ê²°ê³¼ê°’ì„ í™•ë¥ ì²˜ëŸ¼ í•´ì„í•˜ê¸° ìœ„í•´ ì‚¬ìš©.
 (2) : ì€ë‹‰ì¸µ ì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜(sigmoid) : ë ˆì´ì–´ì— ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬. 
"""
# -----------------------------------------------------------------

# 3-1. ì»´íŒŒì¼
loss = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))

# í•™ìŠµë¥ (learning_rate)ì„ ì¡°ì •í•˜ì—¬ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë„ë¡ ë³€ê²½
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(loss)

# í‰ê°€ ì§€í‘œ ì •ì˜
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))

sess = tf.compat.v1.Session()
sess.run(tf.compat.v1.global_variables_initializer())

# 3-2. í›ˆë ¨
epochs = 5001
for step in range(epochs):
    # sess.runì— accuracyë¥¼ ì¶”ê°€í•˜ì—¬ í›ˆë ¨ ê³¼ì •ì—ì„œ ì •í™•ë„ë¥¼ í•¨ê»˜ ê³„ì‚°
    cost_val, acc_val, _ = sess.run([loss, accuracy, train], feed_dict={x: x_data, y: y_data, rate: 0.2})
    
    if step % 500 == 0:
        # lossì™€ í•¨ê»˜ accuracy(acc_val)ë¥¼ ì¶œë ¥
        print(f"Step: {step}, Loss: {cost_val}, Accuracy: {acc_val}")

# 4-1. í‰ê°€
print("\n==== ìµœì¢… í‰ê°€ ====")
# ìµœì¢… í‰ê°€ ì‹œì—ëŠ” ì´ë¯¸ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ train opì€ ì‹¤í–‰í•  í•„ìš”ê°€ ì—†ìŒ
h, p, a = sess.run([hypothesis, predicted, accuracy],
                   feed_dict={x: x_data, y: y_data, rate: 0.0})

print("Hypothesis (ì˜ˆì¸¡ê°’) :\n", h)
print("Predicted (0 ë˜ëŠ” 1) :\n", p)
print("Accuracy (ì •í™•ë„) :", a)
# ==== ìµœì¢… í‰ê°€ ====
# Hypothesis (ì˜ˆì¸¡ê°’) : datatype : Tensor
#  [[0.12889601]
#  [0.9997929 ]
#  [0.9924495 ]
#  [0.1314389 ]]
# Predicted (0 ë˜ëŠ” 1) : datatype : Numpy
#  [[0.]
#  [1.]
#  [1.]
#  [0.]]

sess.close()

# 4-2. ì˜ˆì¸¡
from sklearn.metrics import accuracy_score
import numpy as np

final_acc = accuracy_score((h > 0.5).astype(int), y_data)
# ë˜ëŠ” 
final_acc = accuracy_score(p, y_data)
print('ìµœì¢… acc :', final_acc)  # ìµœì¢… acc : 1.0

### <<65>>

import tensorflow as tf
tf.compat.v1.random.set_random_seed(777)

# 1. ë°ì´í„°
x_data = [[0,0], [0,1], [1,0], [1,1]]   # (2,2)
y_data = [[0], [1], [1], [0]]           # (2,1)

x = tf.compat.v1.placeholder(tf.float32, shape=[None, 2])
y = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])

# w = tf.compat.v1.Variable(tf.random_normal([2,1], name='weights'))
# b = tf.compat.v1.Variable(tf.zeros([1], name='bias'))

# 2. ëª¨ë¸ (MLP êµ¬ì¡°ë¡œ ë³€ê²½)
# -----------------------------------------------------------------
# Layer 1: Hidden Layer (ì€ë‹‰ì¸µ)
# ì…ë ¥ íŠ¹ì„±(feature)ì´ 2ê°œ, íˆë“  ë…¸ë“œì˜ ê°œìˆ˜ë¥¼ 3ê°œë¡œ ì„¤ì •
w1 = tf.compat.v1.Variable(tf.random_normal([2, 3], name='weights1'))
b1 = tf.compat.v1.Variable(tf.zeros([3], name='bias1'))
layer1 = tf.matmul(x, w1) + b1                                  
# (N, 2) * (2, 3) -> (N, 3)


# Layer 2: Hidden Layer (ì€ë‹‰ì¸µ)
# Layer 1ì˜ ì¶œë ¥(3ê°œ)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… ê²°ê³¼(4ê°œ)ë¥¼ ì¶œë ¥
w2 = tf.compat.v1.Variable(tf.random_normal([3, 4], name='weights2'))
b2 = tf.compat.v1.Variable(tf.zeros([4], name='bias2'))
layer2 = tf.compat.v1.sigmoid(tf.matmul(layer1, w2) + b2)
# (N, 3) * (3, 4) -> (N, 4)

# Layer 3: Output Layer (ì¶œë ¥ì¸µ)
# Layer 2ì˜ ì¶œë ¥(4ê°œ)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… ê²°ê³¼(1ê°œ)ë¥¼ ì¶œë ¥
w3 = tf.compat.v1.Variable(tf.random_normal([4, 1], name='weights2'))
b3 = tf.compat.v1.Variable(tf.zeros([1], name='bias2'))
hypothesis = tf.compat.v1.sigmoid(tf.matmul(layer2, w3) + b3)   
# (N, 4) * (4, 1) -> (N, 1)

"""
ğŸŒ [ì¤‘ìš”] ëª¨ë¸ì˜ ë¹„ì„ í˜•ì„± ë¶€ì—¬ëŠ” ì€ë‹‰ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ì— ì˜í•´ì„œ ê²°ì •ëœë‹¤. ì¶œë ¥ì¸µì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ë¹„ì„ í˜•ì„±ê³¼ëŠ” ì•„ë¬´ì‚¬ ìƒê´€ì´ì—†ë‹¤.
-> íˆë“  ë ˆì´ì–´ ì¤‘ì— í•œ ê³³ì´ë¼ë„ sigmoidë¡œ ê°ì‹¸ì„œ ë¹„ì„ í˜•ì„±ì„ ì¤˜ì•¼ accê°€ 1.0ë‹¬ì„±í•œë‹¤.
 (1) : ì¶œë ¥ì¸µ ì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜(sigmoid) : ìµœì¢… ê²°ê³¼ê°’ì„ í™•ë¥ ì²˜ëŸ¼ í•´ì„í•˜ê¸° ìœ„í•´ ì‚¬ìš©.
 (2) : ì€ë‹‰ì¸µ ì—ì„œì˜ í™œì„±í™”í•¨ìˆ˜(sigmoid) : ë ˆì´ì–´ì— ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬. 
"""
# -----------------------------------------------------------------

# 3-1. ì»´íŒŒì¼
loss = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))

# í•™ìŠµë¥ (learning_rate)ì„ ì¡°ì •í•˜ì—¬ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë„ë¡ ë³€ê²½
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(loss)

sess = tf.compat.v1.Session()
sess.run(tf.compat.v1.global_variables_initializer())

# 3-2. í›ˆë ¨
epochs = 5001
for step in range(epochs):
    cost_val, _ = sess.run([loss, train], feed_dict={x: x_data, y: y_data})
    if step % 500 == 0:
        print(step, 'loss :', cost_val)

# 4. í‰ê°€, ì˜ˆì¸¡
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))

# í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ë° ì •í™•ë„ ê³„ì‚°
h, p, a = sess.run([hypothesis, predicted, accuracy],
                   feed_dict={x: x_data, y: y_data})

print("\n==== ìµœì¢… ê²°ê³¼ ====")
print("Hypothesis (ì˜ˆì¸¡ê°’) :\n", h)
print("Predicted (0 ë˜ëŠ” 1) :\n", p)
print("Accuracy (ì •í™•ë„) :", a)
# ==== ìµœì¢… ê²°ê³¼ ====
# Hypothesis (ì˜ˆì¸¡ê°’) :
#  [[0.01365356]
#  [0.9782511 ]
#  [0.98166394]
#  [0.02548812]]
# Predicted (0 ë˜ëŠ” 1) :
#  [[0.]
#  [1.]
#  [1.]
#  [0.]]
# Accuracy (ì •í™•ë„) : 1.0